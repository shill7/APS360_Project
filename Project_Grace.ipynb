{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shill7/APS360_Project/blob/main/Project_Grace.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdB2hi7GOMY0"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import urllib.request\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torchvision.transforms as transforms\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "import shutil\n",
        "\n",
        "import pandas as pd\n",
        "import contractions\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from datasketch import MinHash, MinHashLSH\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install contractions\n",
        "%pip install datasketch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "uG9bEozugx09",
        "outputId": "25e90239-7594-4bce-99f7-aed2c5dddbcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.3-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading anyascii-0.3.3-py3-none-any.whl (345 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.1/345.1 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyahocorasick-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (113 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.9/113.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.3 contractions-0.1.73 pyahocorasick-2.2.0 textsearch-0.0.24\n",
            "Collecting datasketch\n",
            "  Downloading datasketch-1.6.5-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.11/dist-packages (from datasketch) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from datasketch) (1.15.3)\n",
            "Downloading datasketch-1.6.5-py3-none-any.whl (89 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 kB\u001b[0m \u001b[31m472.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: datasketch\n",
            "Successfully installed datasketch-1.6.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_I6s8p9hEgT",
        "outputId": "40ce893e-b3a2-44c7-dfbb-edcd93bb3d92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkZfvp8oOTDC",
        "outputId": "72796a55-a2a2-4d84-8ff0-28fec446edab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading, Preprocessing, and Splitting Dataset"
      ],
      "metadata": {
        "id": "BnYux3Oc41o_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "  # Expand contractions\n",
        "  text = contractions.fix(text)\n",
        "\n",
        "  # Remove non-alphabelic/numeric symbols except basic punctuations\n",
        "  text = re.sub(r'[^\\w\\s.,!?\\'\":;()]', '', text)\n",
        "\n",
        "  # Normalize whitespace\n",
        "  text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "  # Lowercase\n",
        "  text = text.lower()\n",
        "\n",
        "  # Remove short text\n",
        "  if len(text.split()) < 350 or len(nltk.sent_tokenize(text)) < 2:\n",
        "    return None\n",
        "\n",
        "  return text\n",
        "\n",
        "def deduplication(df, text_col='text_clean', threshold=0.9, num_perm=128):\n",
        "  # Initialize MinHashLSH\n",
        "  lsh = MinHashLSH(threshold=threshold, num_perm=num_perm)\n",
        "  minhashes = {}\n",
        "\n",
        "  # Creating MinHash and LSH index\n",
        "  for idx, text in df[text_col].items():\n",
        "    tokens = set(word_tokenize(text))\n",
        "    minhash = MinHash(num_perm=num_perm)\n",
        "    for token in tokens:\n",
        "      minhash.update(token.encode('utf8'))\n",
        "    lsh.insert(idx, minhash)\n",
        "    minhashes[idx] = minhash\n",
        "\n",
        "  # Finding duplicates\n",
        "  remove_data = set()\n",
        "  for idx in df.index:\n",
        "    if idx in remove_data:\n",
        "      continue\n",
        "    duplicates = lsh.query(minhashes[idx])\n",
        "    duplicates = [i for i in duplicates if i != idx]\n",
        "    remove_data.update(duplicates)\n",
        "\n",
        "  deduped_df = df.drop(index=remove_data)\n",
        "  print(f\"Remaining data after deduplication: {len(deduped_df)}\")\n",
        "  return deduped_df"
      ],
      "metadata": {
        "id": "AQDWDSWoha0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def splitDataset(data_path):\n",
        "  # Reading dataset\n",
        "  df = pd.read_csv(os.path.join(data_path, 'Kaggle', 'AI_Human.csv'))\n",
        "\n",
        "  # Clean\n",
        "  df_cleaned = df.copy()\n",
        "  df_cleaned['text_clean'] = df_cleaned['text'].apply(clean_text)\n",
        "  df_cleaned = df_cleaned.dropna(subset=['text_clean'])\n",
        "  print(f\"Remaining data after cleaning: {len(df_cleaned)}\")\n",
        "\n",
        "  # Deduplication\n",
        "  df_deduped = deduplication(df_cleaned)\n",
        "  print(f\"Remaining data after deduplication: {len(df_deduped)}\")\n",
        "\n",
        "  human_df = df_deduped[df_deduped['generated'] == 0]\n",
        "  ai_df = df_deduped[df_deduped['generated'] == 1]\n",
        "\n",
        "  # Getting preprocessed text for human and AI\n",
        "  human_text = human_df['text_clean']\n",
        "  ai_text = ai_df['text_clean']\n",
        "\n",
        "  # Train/Temp Split (70% train, 30% temp)\n",
        "  human_train, human_temp = train_test_split(human_text, test_size=0.30, random_state=42)\n",
        "  ai_train, ai_temp = train_test_split(ai_text, test_size=0.30, random_state=42)\n",
        "  # Val/Test Split (30% temp --> 15% Val, 15% Test)\n",
        "  human_val, human_test = train_test_split(human_temp, test_size=0.50, random_state=42)\n",
        "  ai_val, ai_test = train_test_split(ai_temp, test_size=0.50, random_state=42)\n",
        "\n",
        "  # Save to CSV files\n",
        "  human_train.to_csv(os.path.join(data_path, 'human_train.csv'), index=False, header=True)\n",
        "  human_val.to_csv(os.path.join(data_path, 'human_val.csv'), index=False, header=True)\n",
        "  human_test.to_csv(os.path.join(data_path, 'human_test.csv'), index=False, header=True)\n",
        "\n",
        "  ai_train.to_csv(os.path.join(data_path, 'ai_train.csv'), index=False, header=True)\n",
        "  ai_val.to_csv(os.path.join(data_path, 'ai_val.csv'), index=False, header=True)\n",
        "  ai_test.to_csv(os.path.join(data_path, 'ai_test.csv'), index=False, header=True)\n"
      ],
      "metadata": {
        "id": "XOmTv8aaOa8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loadTrainValTestData(data_path):\n",
        "  # Load data\n",
        "  human_train = pd.read_csv(os.path.join(data_path, 'human_train.csv'))\n",
        "  human_val = pd.read_csv(os.path.join(data_path,'human_val.csv'))\n",
        "  human_test = pd.read_csv(os.path.join(data_path,'human_test.csv'))\n",
        "  ai_train = pd.read_csv(os.path.join(data_path, 'ai_train.csv'))\n",
        "  ai_val = pd.read_csv(os.path.join(data_path, 'ai_val.csv'))\n",
        "  ai_test = pd.read_csv(os.path.join(data_path, 'ai_test.csv'))\n",
        "\n",
        "  # Add labels\n",
        "  human_train['label'] = 0\n",
        "  human_val['label'] = 0\n",
        "  human_test['label'] = 0\n",
        "  ai_train['label'] = 1\n",
        "  ai_val['label'] = 1\n",
        "  ai_test['label'] = 1\n",
        "\n",
        "  return human_train, human_val, human_test, ai_train, ai_val, ai_test"
      ],
      "metadata": {
        "id": "hcmaXtp35Y4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data_path = '/content/drive/My Drive/UofT/APS360 - Project/Data' # Different for everyone\n",
        "# splitDataset(data_path) # Already ran it once"
      ],
      "metadata": {
        "id": "2VhR5_UkF3F1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = '/content/drive/My Drive/UofT/APS360 - Project/Data' # Different for everyone\n",
        "human_train, human_val, human_test, ai_train, ai_val, ai_test = loadTrainValTestData(data_path)\n",
        "print(\"Training set sizes:\")\n",
        "print(\"  Human:\", len(human_train))\n",
        "print(\"  AI:   \", len(ai_train))\n",
        "\n",
        "print(\"\\nValidation set sizes:\")\n",
        "print(\"  Human:\", len(human_val))\n",
        "print(\"  AI:   \", len(ai_val))\n",
        "\n",
        "print(\"\\nTest set sizes:\")\n",
        "print(\"  Human:\", len(human_test))\n",
        "print(\"  AI:   \", len(ai_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7oqc3cHufyb3",
        "outputId": "a4ec8c22-89d8-4bd3-9dde-bb7840d1248a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set sizes:\n",
            "  Human: 56519\n",
            "  AI:    31794\n",
            "\n",
            "Validation set sizes:\n",
            "  Human: 12111\n",
            "  AI:    6813\n",
            "\n",
            "Test set sizes:\n",
            "  Human: 12112\n",
            "  AI:    6814\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"Cars. Cars have been around since they became famous in the 1900s, when Henry Ford created and built the first ModelT. Cars have played a major role in our every day lives since then. But now, people are starting to question if limiting car usage would be a good thing. To me, limiting the use of cars might be a good thing to do.\n",
        "\n",
        "In like matter of this, article, \"In German Suburb, Life Goes On Without Cars,\" by Elizabeth Rosenthal states, how automobiles are the linchpin of suburbs, where middle class families from either Shanghai or Chicago tend to make their homes. Experts say how this is a huge impediment to current efforts to reduce greenhouse gas emissions from tailpipe. Passenger cars are responsible for 12 percent of greenhouse gas emissions in Europe...and up to 50 percent in some carintensive areas in the United States. Cars are the main reason for the greenhouse gas emissions because of a lot of people driving them around all the time getting where they need to go. Article, \"Paris bans driving due to smog,\" by Robert Duffer says, how Paris, after days of nearrecord pollution, enforced a partial driving ban to clear the air of the global city. It also says, how on Monday, motorist with evennumbered license plates were ordered to leave their cars at home or be fined a 22euro fine 31. The same order would be applied to oddnumbered plates the following day. Cars are the reason for polluting entire cities like Paris. This shows how bad cars can be because, of all the pollution that they can cause to an entire city.\n",
        "\n",
        "Likewise, in the article, \"Carfree day is spinning into a big hit in Bogota,\" by Andrew Selsky says, how programs that's set to spread to other countries, millions of Columbians hiked, biked, skated, or took the bus to work during a carfree day, leaving streets of this capital city eerily devoid of traffic jams. It was the third straight year cars have been banned with only buses and taxis permitted for the Day Without Cars in the capital city of 7 million. People like the idea of having carfree days because, it allows them to lesson the pollution that cars put out of their exhaust from people driving all the time. The article also tells how parks and sports centers have bustled throughout the city uneven, pitted sidewalks have been replaced by broad, smooth sidewalks rushhour restrictions have dramatically cut traffic and new restaurants and upscale shopping districts have cropped up. Having no cars has been good for the country of Columbia because, it has aloud them to repair things that have needed repairs for a long time, traffic jams have gone down, and restaurants and shopping districts have popped up, all due to the fact of having less cars around.\n",
        "\n",
        "In conclusion, the use of less cars and having carfree days, have had a big impact on the environment of cities because, it is cutting down the air pollution that the cars have majorly polluted, it has aloud countries like Columbia to repair sidewalks, and cut down traffic jams. Limiting the use of cars would be a good thing for America. So we should limit the use of cars by maybe riding a bike, or maybe walking somewhere that isn't that far from you and doesn't need the use of a car to get you there. To me, limiting the use of cars might be a good thing to do.\"\"\"\n",
        "cleaned = clean_text(text)\n",
        "\n",
        "print(f\"Original text:\\n{text}\\n\")\n",
        "print(f\"Cleaned text:\\n{cleaned}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BM2dAbFGja5S",
        "outputId": "70347e85-bce0-4359-9e16-9241aea840d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text:\n",
            "Cars. Cars have been around since they became famous in the 1900s, when Henry Ford created and built the first ModelT. Cars have played a major role in our every day lives since then. But now, people are starting to question if limiting car usage would be a good thing. To me, limiting the use of cars might be a good thing to do.\n",
            "\n",
            "In like matter of this, article, \"In German Suburb, Life Goes On Without Cars,\" by Elizabeth Rosenthal states, how automobiles are the linchpin of suburbs, where middle class families from either Shanghai or Chicago tend to make their homes. Experts say how this is a huge impediment to current efforts to reduce greenhouse gas emissions from tailpipe. Passenger cars are responsible for 12 percent of greenhouse gas emissions in Europe...and up to 50 percent in some carintensive areas in the United States. Cars are the main reason for the greenhouse gas emissions because of a lot of people driving them around all the time getting where they need to go. Article, \"Paris bans driving due to smog,\" by Robert Duffer says, how Paris, after days of nearrecord pollution, enforced a partial driving ban to clear the air of the global city. It also says, how on Monday, motorist with evennumbered license plates were ordered to leave their cars at home or be fined a 22euro fine 31. The same order would be applied to oddnumbered plates the following day. Cars are the reason for polluting entire cities like Paris. This shows how bad cars can be because, of all the pollution that they can cause to an entire city.\n",
            "\n",
            "Likewise, in the article, \"Carfree day is spinning into a big hit in Bogota,\" by Andrew Selsky says, how programs that's set to spread to other countries, millions of Columbians hiked, biked, skated, or took the bus to work during a carfree day, leaving streets of this capital city eerily devoid of traffic jams. It was the third straight year cars have been banned with only buses and taxis permitted for the Day Without Cars in the capital city of 7 million. People like the idea of having carfree days because, it allows them to lesson the pollution that cars put out of their exhaust from people driving all the time. The article also tells how parks and sports centers have bustled throughout the city uneven, pitted sidewalks have been replaced by broad, smooth sidewalks rushhour restrictions have dramatically cut traffic and new restaurants and upscale shopping districts have cropped up. Having no cars has been good for the country of Columbia because, it has aloud them to repair things that have needed repairs for a long time, traffic jams have gone down, and restaurants and shopping districts have popped up, all due to the fact of having less cars around.\n",
            "\n",
            "In conclusion, the use of less cars and having carfree days, have had a big impact on the environment of cities because, it is cutting down the air pollution that the cars have majorly polluted, it has aloud countries like Columbia to repair sidewalks, and cut down traffic jams. Limiting the use of cars would be a good thing for America. So we should limit the use of cars by maybe riding a bike, or maybe walking somewhere that isn't that far from you and doesn't need the use of a car to get you there. To me, limiting the use of cars might be a good thing to do.\n",
            "\n",
            "Cleaned text:\n",
            "cars. cars have been around since they became famous in the 1900s, when henry ford created and built the first modelt. cars have played a major role in our every day lives since then. but now, people are starting to question if limiting car usage would be a good thing. to me, limiting the use of cars might be a good thing to do. in like matter of this, article, \"in german suburb, life goes on without cars,\" by elizabeth rosenthal states, how automobiles are the linchpin of suburbs, where middle class families from either shanghai or chicago tend to make their homes. experts say how this is a huge impediment to current efforts to reduce greenhouse gas emissions from tailpipe. passenger cars are responsible for 12 percent of greenhouse gas emissions in europe...and up to 50 percent in some carintensive areas in the united states. cars are the main reason for the greenhouse gas emissions because of a lot of people driving them around all the time getting where they need to go. article, \"paris bans driving due to smog,\" by robert duffer says, how paris, after days of nearrecord pollution, enforced a partial driving ban to clear the air of the global city. it also says, how on monday, motorist with evennumbered license plates were ordered to leave their cars at home or be fined a 22euro fine 31. the same order would be applied to oddnumbered plates the following day. cars are the reason for polluting entire cities like paris. this shows how bad cars can be because, of all the pollution that they can cause to an entire city. likewise, in the article, \"carfree day is spinning into a big hit in bogota,\" by andrew selsky says, how programs that is set to spread to other countries, millions of columbians hiked, biked, skated, or took the bus to work during a carfree day, leaving streets of this capital city eerily devoid of traffic jams. it was the third straight year cars have been banned with only buses and taxis permitted for the day without cars in the capital city of 7 million. people like the idea of having carfree days because, it allows them to lesson the pollution that cars put out of their exhaust from people driving all the time. the article also tells how parks and sports centers have bustled throughout the city uneven, pitted sidewalks have been replaced by broad, smooth sidewalks rushhour restrictions have dramatically cut traffic and new restaurants and upscale shopping districts have cropped up. having no cars has been good for the country of columbia because, it has aloud them to repair things that have needed repairs for a long time, traffic jams have gone down, and restaurants and shopping districts have popped up, all due to the fact of having less cars around. in conclusion, the use of less cars and having carfree days, have had a big impact on the environment of cities because, it is cutting down the air pollution that the cars have majorly polluted, it has aloud countries like columbia to repair sidewalks, and cut down traffic jams. limiting the use of cars would be a good thing for america. so we should limit the use of cars by maybe riding a bike, or maybe walking somewhere that is not that far from you and does not need the use of a car to get you there. to me, limiting the use of cars might be a good thing to do.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\"It's official: The electoral college is unfair, outdated, and irrational\" Plumer, Source 2. Many do not like the electoral college for these reasons and many others such as it can be a disaster or because it is just plain dumb. Also there are a few reasons why the electoral college should be kept such as avoiding runoff elections or big states, but those not in favor of it out weigh those in favor of it. The people who despise the electoral college are in favor of popular vote since it is the better choice.\n",
        "\n",
        "For various reasons the electoral college is unfair such as not everyones decisions count just those few people in the electoral college. In a popular vote election everyones vote counts not just those who are considered better than us because they hold authority over people. Those people can also be sneaky and can change votes to be in favor of their choice of president. They will even take bribes sometimes just because they can even though us other people do count.\n",
        "\n",
        "The system should not even be here today because it is outdated way past our time. \"It's hard to say this, but Bob Dole was right: Abolsi the electoral college!\" Plumer, Source 2 and many others do agree with this statement because it rather true that we do so instead of let a bunch of monkeys run our states and country, but I am pretty sure that sometimes they could even do a better job than those in office right now. \".....over 60 percent of voters would prefer a direct election to the kind we have now\" Plumer, Source 2 every day as we continue that percentage continues to grow and that data was recorded in 2000.\n",
        "\n",
        "Lastly, the electoral college is irrational like seriously what idiotic person came up with this. I will say this again, but a monkey could of made a better system than this. \"Under the electoral college system, voters vote not for the president, but for a slate of electors, who in turn elect the president........Who are the electors? They can be anyone not holding public office. Who picks the electors in the first place? It depends on the state. Sometimes state conventions, sometimes the state party's central committee, soemtimes the presidential candidate themselves. Can voters control whom their electors vote for? Not always. DO voters sometimes get confused about the electors and vote for the wrong candiate? Sometimes\" Plumer, Source 2 I know this statement says it all because how could one simply not want popular vote after reading this.\n",
        "\n",
        "I know that electoral college vote can help and not cause problems, but there are more problems while there is one easy fix which is popular vote. \"It's official: The electoral college is unfair, outdated, and irrational\" Plumer, Source 2.\"\"\"\n",
        "\n",
        "cleaned = clean_text(text)\n",
        "\n",
        "print(f\"Original text:\\n{text}\\n\")\n",
        "print(f\"Cleaned text:\\n{cleaned}\")"
      ],
      "metadata": {
        "id": "lSFOUtPSlLdR",
        "outputId": "b6f3e3e7-509f-48de-ccf4-e6ac05434f31",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text:\n",
            "\"It's official: The electoral college is unfair, outdated, and irrational\" Plumer, Source 2. Many do not like the electoral college for these reasons and many others such as it can be a disaster or because it is just plain dumb. Also there are a few reasons why the electoral college should be kept such as avoiding runoff elections or big states, but those not in favor of it out weigh those in favor of it. The people who despise the electoral college are in favor of popular vote since it is the better choice.\n",
            "\n",
            "For various reasons the electoral college is unfair such as not everyones decisions count just those few people in the electoral college. In a popular vote election everyones vote counts not just those who are considered better than us because they hold authority over people. Those people can also be sneaky and can change votes to be in favor of their choice of president. They will even take bribes sometimes just because they can even though us other people do count.\n",
            "\n",
            "The system should not even be here today because it is outdated way past our time. \"It's hard to say this, but Bob Dole was right: Abolsi the electoral college!\" Plumer, Source 2 and many others do agree with this statement because it rather true that we do so instead of let a bunch of monkeys run our states and country, but I am pretty sure that sometimes they could even do a better job than those in office right now. \".....over 60 percent of voters would prefer a direct election to the kind we have now\" Plumer, Source 2 every day as we continue that percentage continues to grow and that data was recorded in 2000.\n",
            "\n",
            "Lastly, the electoral college is irrational like seriously what idiotic person came up with this. I will say this again, but a monkey could of made a better system than this. \"Under the electoral college system, voters vote not for the president, but for a slate of electors, who in turn elect the president........Who are the electors? They can be anyone not holding public office. Who picks the electors in the first place? It depends on the state. Sometimes state conventions, sometimes the state party's central committee, soemtimes the presidential candidate themselves. Can voters control whom their electors vote for? Not always. DO voters sometimes get confused about the electors and vote for the wrong candiate? Sometimes\" Plumer, Source 2 I know this statement says it all because how could one simply not want popular vote after reading this.\n",
            "\n",
            "I know that electoral college vote can help and not cause problems, but there are more problems while there is one easy fix which is popular vote. \"It's official: The electoral college is unfair, outdated, and irrational\" Plumer, Source 2.\n",
            "\n",
            "Cleaned text:\n",
            "\"it is official: the electoral college is unfair, outdated, and irrational\" plumer, source 2. many do not like the electoral college for these reasons and many others such as it can be a disaster or because it is just plain dumb. also there are a few reasons why the electoral college should be kept such as avoiding runoff elections or big states, but those not in favor of it out weigh those in favor of it. the people who despise the electoral college are in favor of popular vote since it is the better choice. for various reasons the electoral college is unfair such as not everyone is decisions count just those few people in the electoral college. in a popular vote election everyone is vote counts not just those who are considered better than us because they hold authority over people. those people can also be sneaky and can change votes to be in favor of their choice of president. they will even take bribes sometimes just because they can even though us other people do count. the system should not even be here today because it is outdated way past our time. \"it is hard to say this, but bob dole was right: abolsi the electoral college!\" plumer, source 2 and many others do agree with this statement because it rather true that we do so instead of let a bunch of monkeys run our states and country, but i am pretty sure that sometimes they could even do a better job than those in office right now. \".....over 60 percent of voters would prefer a direct election to the kind we have now\" plumer, source 2 every day as we continue that percentage continues to grow and that data was recorded in 2000. lastly, the electoral college is irrational like seriously what idiotic person came up with this. i will say this again, but a monkey could of made a better system than this. \"under the electoral college system, voters vote not for the president, but for a slate of electors, who in turn elect the president........who are the electors? they can be anyone not holding public office. who picks the electors in the first place? it depends on the state. sometimes state conventions, sometimes the state party's central committee, soemtimes the presidential candidate themselves. can voters control whom their electors vote for? not always. do voters sometimes get confused about the electors and vote for the wrong candiate? sometimes\" plumer, source 2 i know this statement says it all because how could one simply not want popular vote after reading this. i know that electoral college vote can help and not because problems, but there are more problems while there is one easy fix which is popular vote. \"it is official: the electoral college is unfair, outdated, and irrational\" plumer, source 2.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extracting Features"
      ],
      "metadata": {
        "id": "9B6CxHyw46Xf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extractFeatures(text):\n",
        "  sentences = nltk.sent_tokenize(text)\n",
        "  words_characters = nltk.word_tokenize(text)\n",
        "  words = [word for word in words_characters if word.isalpha()]\n",
        "  stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "  # Features\n",
        "  # 1. Average sentence length\n",
        "  total_words = 0\n",
        "  for s in sentences:\n",
        "    total_words += len(word_tokenize(s))\n",
        "  avg_sentence_length = total_words / len(sentences)\n",
        "\n",
        "  # 2. Average word length\n",
        "  total_characters = 0\n",
        "  for w in words:\n",
        "    total_characters += len(w)\n",
        "  avg_word_length = total_characters / len(words)\n",
        "\n",
        "  # 3. Stopword ratio\n",
        "  stopword_ratio = len([w for w in words if w in stop_words]) / len(words)\n",
        "\n",
        "  # 4. Lexical diversity\n",
        "  lexical_diversity = len(set(words)) / len(words)\n",
        "\n",
        "  return np.array([avg_sentence_length, avg_word_length, stopword_ratio, lexical_diversity], dtype=np.float32)"
      ],
      "metadata": {
        "id": "Y3A8lZlO97QU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = '/content/drive/My Drive/UofT/APS360 - Project/Data' # Different for everyone\n",
        "human_train, human_val, human_test, ai_train, ai_val, ai_test = loadTrainValTestData(data_path)"
      ],
      "metadata": {
        "id": "5N0aapET7g2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.concat([human_train, ai_train], ignore_index=True)\n",
        "val_df = pd.concat([human_val, ai_val], ignore_index=True)\n",
        "test_df = pd.concat([human_test, ai_test], ignore_index=True)\n",
        "\n",
        "X_train = np.stack(train_df['text_clean'].apply(extractFeatures))\n",
        "y_train = train_df['label'].values\n",
        "\n",
        "X_val = np.stack(val_df['text_clean'].apply(extractFeatures))\n",
        "y_val = val_df['label'].values\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)"
      ],
      "metadata": {
        "id": "qirgiIHTFgMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def saveToNumpy(data_path, file, name):\n",
        "  np.save(os.path.join(data_path, f'{name}.npy'), file)\n",
        "\n",
        "def loadFromNumpy(data_path, name):\n",
        "  return np.load(os.path.join(data_path, f'{name}.npy'))\n"
      ],
      "metadata": {
        "id": "7y064gEOH8WV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving and Loading the Extracted Features\n",
        "To avoid reruning the extraction function"
      ],
      "metadata": {
        "id": "vKxoyxKlLrmT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "temp_path = '/content/drive/My Drive/UofT/APS360 - Project/Data/variables'\n",
        "saveToNumpy(temp_path, X_train, 'X_train')\n",
        "saveToNumpy(temp_path, X_train_scaled, 'X_train_scaled')\n",
        "saveToNumpy(temp_path, y_train, 'y_train')\n",
        "saveToNumpy(temp_path, X_val, 'X_val')\n",
        "saveToNumpy(temp_path, X_val_scaled, 'X_val_scaled')\n",
        "saveToNumpy(temp_path, y_val, 'y_val')"
      ],
      "metadata": {
        "id": "eZo7O9BxID3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp_path = '/content/drive/My Drive/UofT/APS360 - Project/Data/variables'\n",
        "X_train2 = loadFromNumpy(temp_path, 'X_train')\n",
        "X_train_scaled2 = loadFromNumpy(temp_path, 'X_train_scaled')\n",
        "y_train2 = loadFromNumpy(temp_path, 'y_train')\n",
        "X_val2 = loadFromNumpy(temp_path, 'X_val')\n",
        "X_val_scaled2 = loadFromNumpy(temp_path, 'X_val_scaled')\n",
        "y_val2 = loadFromNumpy(temp_path, 'y_val')"
      ],
      "metadata": {
        "id": "wXQyFUNELYUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline Model"
      ],
      "metadata": {
        "id": "bZREwBhwE0Oz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters set based on proposal\n",
        "svm_model = SVC(kernel='rbf', C=1.0, gamma='scale')\n",
        "svm_model.fit(X_train_scaled2, y_train2)\n",
        "\n",
        "# Validation\n",
        "val_preds = svm_model.predict(X_val_scaled2)\n",
        "print(\"Validation Results:\\n\", classification_report(y_val2, val_preds, target_names=[\"Human\", \"AI\"]))"
      ],
      "metadata": {
        "id": "VKjxzfrx4-AX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2da8e805-a801-45fc-e15d-a8d9a14408e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Results:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "       Human       0.82      0.95      0.88     12111\n",
            "          AI       0.89      0.63      0.73      6813\n",
            "\n",
            "    accuracy                           0.84     18924\n",
            "   macro avg       0.85      0.79      0.81     18924\n",
            "weighted avg       0.84      0.84      0.83     18924\n",
            "\n"
          ]
        }
      ]
    }
  ]
}