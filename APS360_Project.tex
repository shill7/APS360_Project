\documentclass{article} % For LaTeX2e
\usepackage{iclr2022_conference,times}
% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

%######## APS360: Uncomment your submission name
\newcommand{\apsname}{Project Proposal}
%\newcommand{\apsname}{Progress Report}
%\newcommand{\apsname}{Final Report}

%######## APS360: Put your Group Number here
\newcommand{\gpnumber}{11}

\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{tabularx}
\usepackage{float}

%######## APS360: Put your project Title here
\title{AI Writing Detector: A Deep Learning Approach to Distinguish Human and AI-Generated Text}
 
%######## APS360: Put your names, student IDs and Emails here
\author{Zahra Mohamed Suhail  \\
Student\# 1008997016 \\
\texttt{zahra.suhail@mail.utoronto.ca} \\
\And
Tajrian Islam \\
Student\# 1007939251 \\
\texttt{tajriansyeda.islam@mail.utoronto.ca} \\
\And
Xin Ling (Grace) Li  \\
Student\# 1010143113 \\
\texttt{xlgrace.li@mail.utoronto.ca} \\
\And
Sophia Hill  \\
Student\# 1007865883 \\
\texttt{sophia.hill@mail.utoronto.ca} \\
\And
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy 
\begin{document}

\maketitle
\newpage

\section{Introduction}
With the introduction of ChatGPT in 2022, generative Artificial Intelligence (AI) has become widely accessible, particularly to generate written content at a level similar to that created by humans. However, this quickly raised ethical concerns, as people have begun to pass off AI-written content as their own, which has created issues of plagiarism in educational, professional, and creative spaces. Thus, a need has arisen for a deep learning algorithm that can efficiently and accurately detect AI-written content to mitigate the implications to academic, professional, and artistic integrity, and to prevent inaccurate accusations of plagiarism. Therefore, we propose to create an AI Writing Detector that can accurately detect the complexities of human-written, AI-written, and mixed content. AI algorithms used to generate written content have detectable structures that differ from that of humans. Through a Multi-Layer Perceptron (MLP) model, our team will create an AI Writing Detector that will help resolve the issues of integrity and of the authorship in the age of AI.

\section{Illustration}
Our AI Writing Detector's overall pipeline consists of text preprocessing, feature extractions, and a MLP neural network to classify between AI-generated and human-written text. An illustration of our proposed AI Writing Detector Pipeline is shown in Figure 1.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\linewidth]{illustration.png}
    \caption{An illustration of our proposed AI Writing Detector Pipeline is shown} \citep{Freepik}
    \label{fig:pipeline}
\end{figure}

\section{Background \& Related Work}

\subsection{Related Papers}

\textbf{``Detecting Generative Artificial Intelligence Essays using Large Language Models: Machine and Deep Learning Approaches''} ~\citep{Approaches2024}

This study analyses the efficacy of different deep learning models to identify human written and AI written essays. The algorithms it discusses are logistic regression, Support Vector Machine (SVM), decision trees, random forests, K-Nearest Neighbours (KNN), and Long Short Term Memory (LSTM). The research used Term Frequency-Inverse Document Frequency (TF-IDF) to prepare their data. It found that SVM and LSTM were the most accurate algorithms, though LSTM is more demanding computationally.

\textbf{``Deep learning detection method for large language models-generated scientific content''} \citep{DeepLearning2024}

This paper outlines a model called AI-Catcher developed to identify AI generated writing, citations, and data in scientific papers, treated as a binary classification problem. AI-Catcher uses two models to identify human versus AI generated text: the MLP and a Convolutional Neural Network (CNN). For the MLP, it prepares the data through text cleaning, text encoding, and padding. The MLP model then has five hidden layers, each with 256 neurons, where each layer applies a linear transformation and then ReLU activation. For the CNN model, it prepares the data through extraction of thirteen linguistic and statistical features from the text. Then, it uses the embedding layer to put the encoded integers into vectors, followed by a dropout layer, and then a convolutional layer connected to a global max pooling layer. The results of the two are then concatenated into a single feature vector, which goes through two additional hidden layers. The accuracy, precision, recall, and F1 score of the model were assessed based on confusion matrices. This research was only conducted using scientific papers written by humans or generated by ChatGPT (as opposed to other LLMs).

\textbf{``Detecting AI-generated essays: the ChatGPT challenge''} \citep{AIessays2023}

This research paper from early 2023 investigates effective algorithms to identify human and ChatGPT generated essays. They used a n-gram bag-of-words (BOW) language model for the classifier input with n=5. They tested the performance of Support Vector Machine, Naïve Bayes, Logistic Regression, Random Forest, and Neural Network classification algorithms and then tested the performance of an Ensemble Learning (EL) classifier with the five algorithms fed to it. The findings stated that their SVM, which was modified to eliminate False Negatives entirely, was the most efficient algorithm. The study emphasized eliminating False Negatives (human written essays labelled as AI) due to the ethical implications of incorrectly blaming students for plagiarism with ChatGPT. SVM alone had a slightly lower accuracy than EL, but had better recall and F2 scores. Their SVM model also had fewer False Negatives than OpenAI Detector, GPTZero, and Copyleaks on test data and a 100\% accuracy in detecting human-written essays, despite a lower overall accuracy. This research did also have a relatively small sample size, with 230 total essays for training and 150 for testing (Cingillioglu, 2023).

\subsection{Current Software Solutions}

\textbf{GPTZero} ~\citep{GPTZero}

GPTZero is an AI detector that scans for AI generated writing on a sentence, paragraph, and document level. It can detect AI written content generated by ChatGPT, GPT-4, GPT-3, GPT-2, LLaMA, and derivatives of those models. It claims to have a 99\% accuracy and has an extension that can be added to Google Classroom and Google Docs. It can also identify if a text is entirely human written, entirely AI written, or mixed. However some of its features are locked behind a paid subscription, such as its ability to indicate the most human or AI written parts of a sample of writing and the number of characters, words, or files that can be submitted to it at a time, and it is also primarily for the analysis of English text.

\textbf{Copyleaks} ~\citep{Copyleaks}

Copyleaks is a plagiarism detector and AI detector. It works with 30+ languages and claims to have a 99.8\% accuracy and a 0.2\% false positive rate. It can detect text written by LLMs like ChatGPT, Gemini, DeepSeek, Claude, Jasper 3, LLaMA, and T5. It also shows the percentage of AI in a piece of text and can account for different detection sensitivity levels. It claims to have the capability to identify plagiarism and paraphrasing done with AI and that it can identify human written, AI written, and mixed text. It has a minimum requirement for the number of characters (350) to accurately determine the presence of AI. Copyleaks also has Google Docs extension and mostly requires a paid subscription, with only a limited number of free uses.

\textbf{Grammarly} ~\citep{Grammarly}

Grammarly, an AI based grammar checker, also has its own AI detector. It can detect writing generated by Grammarly, ChatGPT, Google Gemini, and Claude, and it can show the percentage of text that is AI generated. However, it is only available with certain paid subscriptions and they have not stated the accuracy rate of their checker.

\newpage
\section{Data Processing}
Figure 2 illustrates the data process pipeline. It demonstrates the effort to source, repurpose, clean, and structure data rather than just using a pre-existing dataset.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth,height=0.4\textheight,keepaspectratio]{data.png}
    \caption{Schematic overview of the proposed data processing methodology}
    \label{fig:pipeline}
\end{figure}

\subsection{Data Sources}
To ensure a robust and diverse dataset, we will undertake an extensive data collection and processing effort, prioritizing originality and rigor over the use of pre-existing datasets. Our data is sourced from multiple origins to enhance generalization:

\begin{itemize}
    \item \textbf{Human-Written Text:}
    \begin{itemize}
        \item \textbf{Academic Papers \& Essays:}
        \begin{itemize}
            \item Sources: arXiv, PubMed, OpenAlex (for older literature)
            \item Why: High-quality, structured, and varied writing styles
        \end{itemize}
        
        \item \textbf{News Articles:}
        \begin{itemize}
            \item Sources: CBC News, BBC News, Reuters
            \item Why: Factual, professionally edited, diverse topics
        \end{itemize}
        
        \item \textbf{Creative Writing:}
        \begin{itemize}
            \item Sources: Wattpad, Fanfiction.net, literary blogs
            \item Why: Informal, narrative-driven, varied author styles
        \end{itemize}
        
        \item \textbf{Social Media \& Forums:}
        \begin{itemize}
            \item Sources: Reddit (long-form posts), Quora, Facebook
            \item Why: Conversational, personal, and opinionated writing
        \end{itemize}
    \end{itemize}
    
    \item \textbf{AI-Generated Text:}
    \begin{itemize}
        \item GPT Models (OpenAI, Gemini, Deepseek):
        \begin{itemize}
            \item Using the API calls, one can generate synthetic text with varying prompts (e.g., essays, stories, technical writing)
            \item Use different model versions (GPT-3.5, GPT-4, etc.) to capture evolution
        \end{itemize}

         \item Kaggle:
        \begin{itemize}
            \item Provides various styles of texts that one can compare, for example one can fetch a dataset that contains generated texts from LLM.
        \end{itemize}
    \end{itemize}
    
    \item \textbf{Hybrid (Human + AI Edited):}
    \begin{itemize}
        \item Wikipedia Edits:
        \begin{itemize}
            \item By using the edit history one can compare pre- and post-GPT-4 Wikipedia edits
        \end{itemize}
        
        \item GitHub Docs:
        \begin{itemize}
            \item Some repositories use AI for README generation, this can be identified via commit logs
        \end{itemize}
    \end{itemize}
\end{itemize}

\subsection{Data Collection \& Repurposing Efforts}
We gathered data in several careful ways to make sure it was both useful and ethical:

\begin{itemize}
    \item \textbf{Web Scraping (Respectful \& Legal):}
    \begin{itemize}
        \item By using open source frameworks that can help with extracting data in an efficient manner such that delays of overloading servers are avoided, use: \textit{scrapy} or \textit{BeautifulSoup}
        \item Web scraping is legal so long as the data is publicly available
    \end{itemize}
    
    \item \textbf{Using APIs to Generate Synthetic Data:}
    \begin{itemize}
        \item OpenAI API with varied prompts (i.e "Write an essay about AI").
    \end{itemize}
    
    \item \textbf{Data Augmentation:}
    \begin{itemize}
        \item Paraphrase human text using AI to create "AI-like" samples
    \end{itemize}
    
    \item \textbf{Partnerships:}
    \begin{itemize}
        \item Collaborate with universities for student-written vs. AI-assisted essays
    \end{itemize}
\end{itemize}

\subsection{Data Cleaning \& Preprocessing}
Before using the data, we must clean and organize it to ensure quality and fairness. First, we removed duplicate or nearly identical content to avoid repetition:
\begin{itemize}
    \item \textbf{Deduplication \& Noise Removal:}
    \begin{itemize}
        \item Near-Deduplication:
        \begin{itemize}
            \item Use MinHash/LSH (Locality-Sensitive Hashing) to remove near-identical samples
        \end{itemize}
        
        \item Language Filtering:
        \begin{itemize}
            \item Keep only English text (fastText language detection, developed by Facebook AI Researchers).
        \end{itemize}
        
        \item Quality Filtering:
        \begin{itemize}
            \item Remove low-quality text (e.g., incomplete sentences, excessive repetition)
        \end{itemize}
    \end{itemize}
    
    \item \textbf{Text Normalization:}
    \begin{itemize}
        \item Lowercasing (optional, case-sensitive models may retain it)
        \item Remove boilerplate (HTML tags, ads, headers/footers)
        \item Expand contractions ("can't" $\rightarrow$ "cannot") for consistency
    \end{itemize}
    
    \item \textbf{Balancing \& Stratification:}
    \begin{itemize}
        \item Ensure balanced classes (human vs. AI) across various domains of writing (academic, creative, etc.)
    \end{itemize}
\end{itemize}

\subsection{Labeling \& Validation}
There needs to be a way in which human verification is used for labeling and validating the data.

\begin{itemize}
    \item \textbf{Human Verification:}
    \begin{itemize}
        \item Crowdsource (Amazon Mechanical Turk) to label ambiguous cases
        \item Expert review for disputed samples (e.g., linguistics PhDs)
    \end{itemize}
    
    \item \textbf{Confidence Scoring:}
    \begin{itemize}
        \item Use models (RoBERTa, GPT-detectors) to flag uncertain samples for rechecking
    \end{itemize}

    \item \textbf{Kaggle:}
    \begin{itemize}
        \item By using a dataset of LLM generated texts, provided from this source one can compare and contrast when validating data.
    \end{itemize}
\end{itemize}

\subsection{Feature Engineering}
\begin{itemize}
    \item \textbf{Stylometric Features:}
    \begin{itemize}
        \item Sentence length variance, word rarity, POS tag ratios
    \end{itemize}
    
    \item \textbf{Perplexity Scores:}
    \begin{itemize}
        \item Compare human vs. AI text using small GPT-2
    \end{itemize}
    
    \item \textbf{Watermarking Detection:}
    \begin{itemize}
        \item If watermarks are presented in the AI text(e.g., OpenAI's), incorporate detection.
    \end{itemize}
\end{itemize}

\subsection{Dataset Splitting}
\begin{itemize}
    \item Split data into: Train (70\%) / Validation (15\%) / Test (15\%)
    \item Ensure each split has no overlapping sources/authors.
    \item If testing newer AI Models. One can split based on time (e.g., train on GPT-3, test on GPT-4).
\end{itemize}

\section{Architecture}
The MLP stands out as the most effective neural network architecture for AI text detection. Studies comparing various machine learning models have shown that MLPs strike the best balance between accuracy, efficiency, and flexibility. While SVMs require manual feature engineering and LSTM networks are computationally intensive, MLPs deliver high accuracy without these drawbacks. As highlighted in the AI-Catcher study, MLPs outperform CNNs in capturing the overall coherence of text, which is a key factor in detecting AI-generated content. Unlike complex ensemble methods, which added little value in challenges like ChatGPT detection, MLPs offer a simpler, more interpretable solution that scales well. MLPs further enhance their practicality by working with both engineered features and raw text embeddings. Altogether, MLPs combine performance, simplicity, and real-world validation, making them an ideal choice for reliable, production-ready AI text detection system.

\section{Baseline Model}
Most of the research that investigates effective AI writing detection algorithms found that SVM was the best model for accuracy.
Sklearn has a built-in SVM model package that can be built given specific parameters and trained on the data. This would require hyperparameter tuning of the model, which would initially be based off of what was found to be effective in the literature. A 2024 study focused on the detection of human-written and AI-written texts found the default settings of Sklearn's SVM model were sufficient in training (Tariq et al., 2024). With the default settings, the kernel will be "rbf" (Radial Basis Function) for non-linear classification, the regularization parameter will be set to 1.0, and the gamma parameter set to "scale" (Tariq et al., 2024).

\section{Ethical Considerations}
When creating an AI detection tool, there are many ethical concerns such as accuracy, bias, and privacy.
While we aim to make a model as accurate as possible, there is still a chance for human writing to be detected as AI. Bias in the model is another issue, which can occur from the training data. A group particularly affected by AI detection systems is that of non-native English speakers, whose writing is flagged as AI-generated more often than that of native speakers due to differences in writing styles and levels of sophistication (Myers, 2023). Also, the risk of a biased model is that individuals begin to fear being flagged, even without AI usage. Such fears can stop people from openly expressing themselves and limit creativity. Another ethical consideration is that many individuals may not be comfortable in having their work used for model training, and the use of their data without consent would be a breach of privacy. 
We need to be careful in training our AI model as there are a lot of consequences of false detection of AI generated writing since plagiarism is tied into a person’s credibility. With a larger dataset and the ability to learn from a variety of different texts, the team will aim to train the model to be as accurate as possible and reduce its bias.

\section{Project Plan}
We will hold weekly meetings on Mondays to discuss current progress and future steps. All forms of communications will take place online via Discord, unless specified otherwise. To ensure we do not overwrite each other's code, each member will create and work on their own branches through the use of version control. Each member will open a pull request when they wish to merge with the main branch after code reviews have been completed. 

A summary of the Project Plan timeline with assignees and deadlines is shown in Figure 3. 
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\linewidth,height=0.9\textheight,keepaspectratio]{gnatt.png}
    \caption{Project Gantt Chart}
    \label{fig:pipeline}
\end{figure}

\section{Risk Register}
In the creation of an AI model to detect if written work is made by AI, there are many problems that the team may encounter. Despite the chance of these challenges occurring, we have planned ahead on how to handle them.

\subsection{High Risk — Long Training Time Due to Large Dataset}
\textbf{Risk:} Since our dataset has a large dataset, each with at least 350 words entered at a time, training the model on the full set is likely to take a significant amount of time.\\
\textbf{Solution:} To mitigate the risk of not completing the training on time, we intend to reduce our sample size accordingly, potentially by half, ensuring that our set is still diverse. If time allows later on, we can expand the dataset for further training. 

\subsection{Medium Risk — Breach of User Privacy}
\textbf{Risk:} Since our model will be trained on written content, there is a chance that users may not want their own work to be used, making the likelihood of invading user privacy medium risk.\\
\textbf{Solution:} To mitigate any breach of privacy, we will warn users of how their data is being used. That way, whoever uses the model is giving their consent for their work to be used for training purposes and they are aware of how the data they are giving is being handled.

\subsection{Medium Risk — Detecting Human Writing as AI in Plagiarism Detection}
\textbf{Risk:} As the model is limited to the training of our dataset, the likelihood that it becomes biased and can mistake human writing as AI is a medium risk. \\
\textbf{Solution:} To mitigate the chance of the model mistaking human writing as AI, we have a large dataset with a variety of writing, teaching the model as best as possible in detecting if the written work was done by AI. This way, the model can learn from a more complex dataset and be better prepared in differentiating AI-written work from human-written work.

\subsection{Medium Risk — Missing Deadlines}
\textbf{Risk:} Our members have busy schedules with many of us working or having other activities to take care of, making the likelihood of missing deadlines a medium risk. \\
\textbf{Solution:} To mitigate the chance of missing major deadlines for the project or procrastination, the team will break tasks up and create internal deadlines using gantt charts. That way if any team member is unable to complete their part on time, there is still time for other members to help them and meet the major deadline.

\subsection{Low Risk — Team Members Dropping the Course}
\textbf{Risk:} As all of our team members need this course for our degrees, the likelihood of anyone dropping out is low risk. \\
\textbf{Solution:} To mitigate the damage to the project timeline if a team member were to drop, the team member that is leaving must notify the group as soon as possible. The team member leaving should ensure others understand the next steps of any incomplete work. Also having gantt charts and weekly meetings will help the other members redistribute responsibilities.

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    urlcolor=blue
}

\section{Link to Github/Colab}
\noindent
\href{https://github.com/shill7/APS360_Project}{\underline{\textcolor{blue}{GitHub Repository}}}

\label{last_page}

\bibliography{APS360_ref}
\bibliographystyle{iclr2022_conference}

\end{document}