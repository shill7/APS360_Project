{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shill7/APS360_Project/blob/main/Project_Sophia.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install contractions\n",
        "%pip install datasketch"
      ],
      "metadata": {
        "id": "81a7a-Ji2-Bu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b95205c0-3d9f-4317-e5f7-de69e32cd59a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: contractions in /usr/local/lib/python3.11/dist-packages (0.1.73)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.11/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (0.3.3)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (2.2.0)\n",
            "Requirement already satisfied: datasketch in /usr/local/lib/python3.11/dist-packages (1.6.5)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.11/dist-packages (from datasketch) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from datasketch) (1.15.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "bhPQPOBYCsfq"
      },
      "outputs": [],
      "source": [
        "import sklearn\n",
        "from sklearn import svm\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import urllib.request\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torchvision.transforms as transforms\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "import shutil\n",
        "import string\n",
        "import re\n",
        "\n",
        "import pandas as pd\n",
        "import contractions\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from datasketch import MinHash, MinHashLSH\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "YFvD7bImYcrK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78dcb29f-cc23-4d0b-db59-b45153090238"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('cmudict')"
      ],
      "metadata": {
        "id": "84ijL9bL2_W2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc7ed7b5-9525-4cab-f2c7-d427bac7230c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/cmudict.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import cmudict\n",
        "d = cmudict.dict()"
      ],
      "metadata": {
        "id": "NJ1TgLtg2CTB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Preprocessing and Splitting Dataset**"
      ],
      "metadata": {
        "id": "OJgVE-aOYd8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "  # Expand contractions\n",
        "  text = contractions.fix(text)\n",
        "\n",
        "  # Remove non-alphabelic/numeric symbols except basic punctuations\n",
        "  text = re.sub(r'[^\\w\\s.,!?\\'\":;()]', '', text)\n",
        "\n",
        "  # Normalize whitespace\n",
        "  text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "  # Lowercase\n",
        "  text = text.lower()\n",
        "\n",
        "  # Remove short text\n",
        "  if len(text.split()) < 350 or len(nltk.sent_tokenize(text)) < 2:\n",
        "    return None\n",
        "\n",
        "  return text\n",
        "\n",
        "def deduplication(df, text_col='text_clean', threshold=0.9, num_perm=128):\n",
        "  # Initialize MinHashLSH\n",
        "  lsh = MinHashLSH(threshold=threshold, num_perm=num_perm)\n",
        "  minhashes = {}\n",
        "\n",
        "  # Creating MinHash and LSH index\n",
        "  for idx, text in df[text_col].items():\n",
        "    tokens = set(word_tokenize(text))\n",
        "    m = MinHash(num_perm=num_perm)\n",
        "    for token in tokens:\n",
        "      m.update(token.encode('utf8'))\n",
        "    lsh.insert(idx, m)\n",
        "    minhashes[idx] = m\n",
        "\n",
        "  # Finding duplicates\n",
        "  remove_data = set()\n",
        "  for idx in df.index:\n",
        "    if idx in remove_data:\n",
        "      continue\n",
        "    near_dupes = lsh.query(minhashes[idx])\n",
        "    near_dupes = [i for i in near_dupes if i != idx]\n",
        "    remove_data.update(near_dupes)\n",
        "\n",
        "  deduped_df = df.drop(index=remove_data)\n",
        "  return deduped_df"
      ],
      "metadata": {
        "id": "c9P5-Jbj3CLu"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def splitDataset(data_path):\n",
        "  # Reading dataset\n",
        "  df = pd.read_csv(os.path.join(data_path, 'Kaggle', 'AI_Human.csv'))\n",
        "\n",
        "  # Clean\n",
        "  df_cleaned = df.copy()\n",
        "  df_cleaned['text_clean'] = df_cleaned['text'].apply(clean_text)\n",
        "  df_cleaned = df_cleaned.dropna(subset=['text_clean'])\n",
        "  print(f\"Remaining data after cleaning: {len(df_cleaned)}\")\n",
        "\n",
        "  # Deduplication\n",
        "  df_deduped = deduplication(df_cleaned)\n",
        "  print(f\"Remaining data after deduplication: {len(df_deduped)}\")\n",
        "\n",
        "  human_df = df_deduped[df_deduped['generated'] == 0]\n",
        "  ai_df = df_deduped[df_deduped['generated'] == 1]\n",
        "\n",
        "  # Getting preprocessed text for human and AI\n",
        "  human_text = human_df['text_clean']\n",
        "  ai_text = ai_df['text_clean']\n",
        "\n",
        "  # Train/Temp Split (70% train, 30% temp)\n",
        "  human_train, human_temp = train_test_split(human_text, test_size=0.30, random_state=42)\n",
        "  ai_train, ai_temp = train_test_split(ai_text, test_size=0.30, random_state=42)\n",
        "  # Val/Test Split (30% temp --> 15% Val, 15% Test)\n",
        "  human_val, human_test = train_test_split(human_temp, test_size=0.50, random_state=42)\n",
        "  ai_val, ai_test = train_test_split(ai_temp, test_size=0.50, random_state=42)\n",
        "\n",
        "  # Save to CSV files\n",
        "  human_train.to_csv(os.path.join(data_path, 'human_train.csv'), index=False, header=True)\n",
        "  human_val.to_csv(os.path.join(data_path, 'human_val.csv'), index=False, header=True)\n",
        "  human_test.to_csv(os.path.join(data_path, 'human_test.csv'), index=False, header=True)\n",
        "\n",
        "  ai_train.to_csv(os.path.join(data_path, 'ai_train.csv'), index=False, header=True)\n",
        "  ai_val.to_csv(os.path.join(data_path, 'ai_val.csv'), index=False, header=True)\n",
        "  ai_test.to_csv(os.path.join(data_path, 'ai_test.csv'), index=False, header=True)\n"
      ],
      "metadata": {
        "id": "WpXe1Gj0YmOG"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loadTrainValTestData(data_path):\n",
        "  # Load data\n",
        "  human_train = pd.read_csv(os.path.join(data_path, 'human_train.csv'))\n",
        "  human_val = pd.read_csv(os.path.join(data_path,'human_val.csv'))\n",
        "  human_test = pd.read_csv(os.path.join(data_path,'human_test.csv'))\n",
        "  ai_train = pd.read_csv(os.path.join(data_path, 'ai_train.csv'))\n",
        "  ai_val = pd.read_csv(os.path.join(data_path, 'ai_val.csv'))\n",
        "  ai_test = pd.read_csv(os.path.join(data_path, 'ai_test.csv'))\n",
        "\n",
        "  # Add labels\n",
        "  human_train['label'] = 0\n",
        "  human_val['label'] = 0\n",
        "  human_test['label'] = 0\n",
        "  ai_train['label'] = 1\n",
        "  ai_val['label'] = 1\n",
        "  ai_test['label'] = 1\n",
        "\n",
        "  return human_train, human_val, human_test, ai_train, ai_val, ai_test"
      ],
      "metadata": {
        "id": "2jooByygXtiQ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#--------- Splitting the data ------------\n",
        "data_path = '/content/drive/My Drive/UofT/APS360 - Project/Data' # Different for everyone\n",
        "# splitDataset(data_path)"
      ],
      "metadata": {
        "id": "IrdBqKbSMh6b"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Extracting Features**\n"
      ],
      "metadata": {
        "id": "VnrDaMKKZm7S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_syllables_word(word):\n",
        "    word_lower = word.lower()\n",
        "    if word_lower in d:\n",
        "        # CMUdict provides pronunciations as lists of phonemes.\n",
        "        # Syllables are typically marked by digits at the end of phonemes.\n",
        "        return [len(list(y for y in x if y[-1].isdigit())) for x in d[word_lower]][0]\n",
        "    else:\n",
        "        # Handle words not found in CMUdict (e.g., by estimating based on vowels)\n",
        "        # This is a simplified fallback; more robust methods exist.\n",
        "        vowels = \"aeiouy\"\n",
        "        count = 0\n",
        "        if word_lower[0] in vowels:\n",
        "            count += 1\n",
        "        for index in range(1, len(word_lower)):\n",
        "            if word_lower[index] in vowels and word_lower[index - 1] not in vowels:\n",
        "                 count += 1\n",
        "        if word_lower.endswith(\"e\"):\n",
        "            count -= 1\n",
        "        if count == 0:\n",
        "             count += 1\n",
        "        return count"
      ],
      "metadata": {
        "id": "8P43pi7K2S-8"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_total_syllables_text(text):\n",
        "    total_syllables = 0\n",
        "    # Split text into words, removing punctuation\n",
        "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "    for word in words:\n",
        "        total_syllables += count_syllables_word(word)\n",
        "    return total_syllables"
      ],
      "metadata": {
        "id": "d2D_BrZR2ugp"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extractFeatures(text):\n",
        "  sentences = nltk.sent_tokenize(text)\n",
        "  words_characters = nltk.word_tokenize(text)\n",
        "  words = [word for word in words_characters if word.isalpha()]\n",
        "  stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "  # characters =\n",
        "\n",
        "  # Features\n",
        "  # 1. Average sentence length\n",
        "  total_words = 0\n",
        "  for s in sentences:\n",
        "    total_words += len(word_tokenize(s))\n",
        "  avg_sentence_length = total_words / len(sentences)\n",
        "\n",
        "  # 2. Average word length\n",
        "  total_characters = 0\n",
        "  for w in words:\n",
        "    total_characters += len(w)\n",
        "  avg_word_length = total_characters / len(words)\n",
        "\n",
        "  # 3. Stopword ratio\n",
        "  stopword_ratio = len([w for w in words if w in stop_words]) / len(words)\n",
        "\n",
        "  # 4. Lexical diversity\n",
        "  lexical_diversity = len(set(words)) / len(words)\n",
        "\n",
        "  # 5. Punctuation count\n",
        "  punctuation_counts = {}\n",
        "  for char in text:\n",
        "       if char in string.punctuation:\n",
        "          punctuation_counts[char] = punctuation_counts.get(char, 0) + 1\n",
        "\n",
        "  total_punctuation_occurrences = sum(punctuation_counts.values())\n",
        "  num_unique_punctuation_types = len(punctuation_counts)\n",
        "  avg_punc = total_punctuation_occurrences / num_unique_punctuation_types\n",
        "\n",
        "  # 6. Readability Score\n",
        "  # Flesch Reading Ease\n",
        "  flesch = 206.835 - (1.015 * avg_sentence_length) - (84.6 * (count_total_syllables_text(text) / total_words))\n",
        "\n",
        "  return np.array([avg_sentence_length, avg_word_length, stopword_ratio, lexical_diversity, total_punctuation_occurrences, num_unique_punctuation_types, avg_punc, flesch], dtype=np.float32)"
      ],
      "metadata": {
        "id": "QyeBZYM9GA2R"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = '/content/drive/My Drive/UofT/Third Year/APS360/APS360 - Project/Data' # Different for everyone\n",
        "\n",
        "#--------- Loading the datasets ------------\n",
        "human_train, human_val, human_test, ai_train, ai_val, ai_test = loadTrainValTestData(data_path)"
      ],
      "metadata": {
        "id": "uaZv-Q9qYVkS"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.concat([human_train, ai_train], ignore_index=True)\n",
        "val_df = pd.concat([human_val, ai_val], ignore_index=True)\n",
        "test_df = pd.concat([human_test, ai_test], ignore_index=True)\n",
        "\n",
        "# Extracting features\n",
        "X_train = np.stack(train_df['text_clean'].apply(extractFeatures))\n",
        "y_train = train_df['label'].values\n",
        "\n",
        "X_val = np.stack(val_df['text_clean'].apply(extractFeatures))\n",
        "y_val = val_df['label'].values\n",
        "\n",
        "# Standarizing the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)"
      ],
      "metadata": {
        "id": "30B7kKg9ZbXy"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving and Loading the Extracted Features\n",
        "To avoid reruning the extraction function"
      ],
      "metadata": {
        "id": "g4n20wg0MVqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def saveToNumpy(data_path, file, name):\n",
        "  np.save(os.path.join(data_path, f'{name}.npy'), file)\n",
        "\n",
        "def loadFromNumpy(data_path, name):\n",
        "  return np.load(os.path.join(data_path, f'{name}.npy'))"
      ],
      "metadata": {
        "id": "DCW-JrxzMT-7"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp_path = '/content/drive/My Drive/UofT/Third Year/APS360/APS360 - Project/Data/variables'\n",
        "saveToNumpy(temp_path, X_train, 'X_train')\n",
        "saveToNumpy(temp_path, X_train_scaled, 'X_train_scaled')\n",
        "saveToNumpy(temp_path, y_train, 'y_train')\n",
        "saveToNumpy(temp_path, X_val, 'X_val')\n",
        "saveToNumpy(temp_path, X_val_scaled, 'X_val_scaled')\n",
        "saveToNumpy(temp_path, y_val, 'y_val')"
      ],
      "metadata": {
        "id": "SBv_1h0lMXwC"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp_path = '/content/drive/My Drive/UofT/Third Year/APS360/APS360 - Project/Data/variables'\n",
        "X_train2 = loadFromNumpy(temp_path, 'X_train')\n",
        "X_train_scaled2 = loadFromNumpy(temp_path, 'X_train_scaled')\n",
        "y_train2 = loadFromNumpy(temp_path, 'y_train')\n",
        "X_val2 = loadFromNumpy(temp_path, 'X_val')\n",
        "X_val_scaled2 = loadFromNumpy(temp_path, 'X_val_scaled')\n",
        "y_val2 = loadFromNumpy(temp_path, 'y_val')"
      ],
      "metadata": {
        "id": "635NEKmkMZV8"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Baseline Model**"
      ],
      "metadata": {
        "id": "jJ6q8j1_Zszy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters set based on proposal\n",
        "# Build the model\n",
        "svm = SVC(kernel='rbf', C=1.0, gamma='scale')\n",
        "# Trained the model\n",
        "svm.fit(X_train_scaled2, y_train2)\n",
        "\n",
        "# Validation\n",
        "val_preds = svm.predict(X_val_scaled2)\n",
        "print(\"Validation Results:\\n\", classification_report(y_val2, val_preds, target_names=[\"Human\", \"AI\"]))"
      ],
      "metadata": {
        "id": "ntgaHH7xZvlJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d37c629c-d236-4bca-eef1-001cf7a23b6c"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Results:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "       Human       0.88      0.96      0.91     12111\n",
            "          AI       0.91      0.76      0.83      6813\n",
            "\n",
            "    accuracy                           0.88     18924\n",
            "   macro avg       0.89      0.86      0.87     18924\n",
            "weighted avg       0.89      0.88      0.88     18924\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO9iEdFd9w3lXFqYZAM5dL6",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}