{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shill7/APS360_Project/blob/main/Project_Master.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install contractions\n",
        "%pip install datasketch"
      ],
      "metadata": {
        "id": "81a7a-Ji2-Bu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e459b75-6d1f-413c-b43f-448ac195febb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.3-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading anyascii-0.3.3-py3-none-any.whl (345 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.1/345.1 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyahocorasick-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (113 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.9/113.9 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.3 contractions-0.1.73 pyahocorasick-2.2.0 textsearch-0.0.24\n",
            "Collecting datasketch\n",
            "  Downloading datasketch-1.6.5-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.11/dist-packages (from datasketch) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from datasketch) (1.15.3)\n",
            "Downloading datasketch-1.6.5-py3-none-any.whl (89 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: datasketch\n",
            "Successfully installed datasketch-1.6.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhPQPOBYCsfq"
      },
      "outputs": [],
      "source": [
        "import sklearn\n",
        "from sklearn import svm\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import urllib.request\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torchvision.transforms as transforms\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "import shutil\n",
        "import string\n",
        "import re\n",
        "\n",
        "import pandas as pd\n",
        "import contractions\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from datasketch import MinHash, MinHashLSH\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "YFvD7bImYcrK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fc8a1ee-b027-444d-b5b8-972b0cd16dfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('cmudict')"
      ],
      "metadata": {
        "id": "84ijL9bL2_W2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6956763-462d-4879-bebe-bf3d153df6ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import cmudict\n",
        "d = cmudict.dict()"
      ],
      "metadata": {
        "id": "NJ1TgLtg2CTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Preprocessing and Splitting Dataset**"
      ],
      "metadata": {
        "id": "OJgVE-aOYd8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "  # Expand contractions\n",
        "  text = contractions.fix(text)\n",
        "\n",
        "  # Remove non-alphabelic/numeric symbols except basic punctuations\n",
        "  text = re.sub(r'[^\\w\\s.,!?\\'\":;()]', '', text)\n",
        "\n",
        "  # Normalize whitespace\n",
        "  text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "  # Lowercase\n",
        "  text = text.lower()\n",
        "\n",
        "  # Remove short text\n",
        "  if len(text.split()) < 350 or len(nltk.sent_tokenize(text)) < 2:\n",
        "    return None\n",
        "\n",
        "  return text\n",
        "\n",
        "def deduplication(df, text_col='text_clean', threshold=0.9, num_perm=128):\n",
        "  # Initialize MinHashLSH\n",
        "  lsh = MinHashLSH(threshold=threshold, num_perm=num_perm)\n",
        "  minhashes = {}\n",
        "\n",
        "  # Creating MinHash and LSH index\n",
        "  for idx, text in df[text_col].items():\n",
        "    tokens = set(word_tokenize(text))\n",
        "    m = MinHash(num_perm=num_perm)\n",
        "    for token in tokens:\n",
        "      m.update(token.encode('utf8'))\n",
        "    lsh.insert(idx, m)\n",
        "    minhashes[idx] = m\n",
        "\n",
        "  # Finding duplicates\n",
        "  remove_data = set()\n",
        "  for idx in df.index:\n",
        "    if idx in remove_data:\n",
        "      continue\n",
        "    near_dupes = lsh.query(minhashes[idx])\n",
        "    near_dupes = [i for i in near_dupes if i != idx]\n",
        "    remove_data.update(near_dupes)\n",
        "\n",
        "  deduped_df = df.drop(index=remove_data)\n",
        "  return deduped_df"
      ],
      "metadata": {
        "id": "c9P5-Jbj3CLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def splitDataset(data_path):\n",
        "  # Reading dataset\n",
        "  df = pd.read_csv(os.path.join(data_path, 'Kaggle', 'AI_Human.csv'))\n",
        "\n",
        "  # Clean\n",
        "  df_cleaned = df.copy()\n",
        "  df_cleaned['text_clean'] = df_cleaned['text'].apply(clean_text)\n",
        "  df_cleaned = df_cleaned.dropna(subset=['text_clean'])\n",
        "  print(f\"Remaining data after cleaning: {len(df_cleaned)}\")\n",
        "\n",
        "  # Deduplication\n",
        "  df_deduped = deduplication(df_cleaned)\n",
        "  print(f\"Remaining data after deduplication: {len(df_deduped)}\")\n",
        "\n",
        "  human_df = df_deduped[df_deduped['generated'] == 0]\n",
        "  ai_df = df_deduped[df_deduped['generated'] == 1]\n",
        "\n",
        "  # Getting preprocessed text for human and AI\n",
        "  human_text = human_df['text_clean']\n",
        "  ai_text = ai_df['text_clean']\n",
        "\n",
        "  # Train/Temp Split (70% train, 30% temp)\n",
        "  human_train, human_temp = train_test_split(human_text, test_size=0.30, random_state=42)\n",
        "  ai_train, ai_temp = train_test_split(ai_text, test_size=0.30, random_state=42)\n",
        "  # Val/Test Split (30% temp --> 15% Val, 15% Test)\n",
        "  human_val, human_test = train_test_split(human_temp, test_size=0.50, random_state=42)\n",
        "  ai_val, ai_test = train_test_split(ai_temp, test_size=0.50, random_state=42)\n",
        "\n",
        "  # Save to CSV files\n",
        "  human_train.to_csv(os.path.join(data_path, 'human_train.csv'), index=False, header=True)\n",
        "  human_val.to_csv(os.path.join(data_path, 'human_val.csv'), index=False, header=True)\n",
        "  human_test.to_csv(os.path.join(data_path, 'human_test.csv'), index=False, header=True)\n",
        "\n",
        "  ai_train.to_csv(os.path.join(data_path, 'ai_train.csv'), index=False, header=True)\n",
        "  ai_val.to_csv(os.path.join(data_path, 'ai_val.csv'), index=False, header=True)\n",
        "  ai_test.to_csv(os.path.join(data_path, 'ai_test.csv'), index=False, header=True)\n"
      ],
      "metadata": {
        "id": "WpXe1Gj0YmOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loadTrainValTestData(data_path):\n",
        "  # Load data\n",
        "  human_train = pd.read_csv(os.path.join(data_path, 'human_train.csv'))\n",
        "  human_val = pd.read_csv(os.path.join(data_path,'human_val.csv'))\n",
        "  human_test = pd.read_csv(os.path.join(data_path,'human_test.csv'))\n",
        "  ai_train = pd.read_csv(os.path.join(data_path, 'ai_train.csv'))\n",
        "  ai_val = pd.read_csv(os.path.join(data_path, 'ai_val.csv'))\n",
        "  ai_test = pd.read_csv(os.path.join(data_path, 'ai_test.csv'))\n",
        "\n",
        "  # Add labels\n",
        "  human_train['label'] = 0\n",
        "  human_val['label'] = 0\n",
        "  human_test['label'] = 0\n",
        "  ai_train['label'] = 1\n",
        "  ai_val['label'] = 1\n",
        "  ai_test['label'] = 1\n",
        "\n",
        "  return human_train, human_val, human_test, ai_train, ai_val, ai_test"
      ],
      "metadata": {
        "id": "2jooByygXtiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#--------- Splitting the data ------------\n",
        "data_path = '/content/drive/My Drive/UofT/APS360 - Project/Data' # Different for everyone\n",
        "# splitDataset(data_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrdBqKbSMh6b",
        "outputId": "839270fb-288d-42d7-e585-494054eea627"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Remaining data after cleaning: 267749\n",
            "Remaining data after deduplication: 126163\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Extracting Features**\n"
      ],
      "metadata": {
        "id": "VnrDaMKKZm7S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_syllables_word(word):\n",
        "    word_lower = word.lower()\n",
        "    if word_lower in d:\n",
        "        # CMUdict provides pronunciations as lists of phonemes.\n",
        "        # Syllables are typically marked by digits at the end of phonemes.\n",
        "        return [len(list(y for y in x if y[-1].isdigit())) for x in d[word_lower]][0]\n",
        "    else:\n",
        "        # Handle words not found in CMUdict (e.g., by estimating based on vowels)\n",
        "        # This is a simplified fallback; more robust methods exist.\n",
        "        vowels = \"aeiouy\"\n",
        "        count = 0\n",
        "        if word_lower[0] in vowels:\n",
        "            count += 1\n",
        "        for index in range(1, len(word_lower)):\n",
        "            if word_lower[index] in vowels and word_lower[index - 1] not in vowels:\n",
        "                 count += 1\n",
        "        if word_lower.endswith(\"e\"):\n",
        "            count -= 1\n",
        "        if count == 0:\n",
        "             count += 1\n",
        "        return count"
      ],
      "metadata": {
        "id": "8P43pi7K2S-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_total_syllables_text(text):\n",
        "    total_syllables = 0\n",
        "    # Split text into words, removing punctuation\n",
        "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "    for word in words:\n",
        "        total_syllables += count_syllables_word(word)\n",
        "    return total_syllables"
      ],
      "metadata": {
        "id": "d2D_BrZR2ugp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extractFeatures(text):\n",
        "  sentences = nltk.sent_tokenize(text)\n",
        "  words_characters = nltk.word_tokenize(text)\n",
        "  words = [word for word in words_characters if word.isalpha()]\n",
        "  stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "  # Features\n",
        "  # 1. Average sentence length\n",
        "  total_words = 0\n",
        "  for s in sentences:\n",
        "    total_words += len(word_tokenize(s))\n",
        "  avg_sentence_length = total_words / len(sentences)\n",
        "\n",
        "  # 2. Average word length\n",
        "  total_characters = 0\n",
        "  for w in words:\n",
        "    total_characters += len(w)\n",
        "  avg_word_length = total_characters / len(words)\n",
        "\n",
        "  # 3. Stopword ratio\n",
        "  stopword_ratio = len([w for w in words if w in stop_words]) / len(words)\n",
        "\n",
        "  # 4. Lexical diversity\n",
        "  lexical_diversity = len(set(words)) / len(words)\n",
        "\n",
        "  # 5. Punctuation count\n",
        "  punctuation_counts = {}\n",
        "  for char in text:\n",
        "       if char in string.punctuation:\n",
        "          punctuation_counts[char] = punctuation_counts.get(char, 0) + 1\n",
        "\n",
        "  total_punctuation_occurrences = sum(punctuation_counts.values())\n",
        "  num_unique_punctuation_types = len(punctuation_counts)\n",
        "  avg_punc = total_punctuation_occurrences / num_unique_punctuation_types\n",
        "\n",
        "  # 6. Readability Score\n",
        "  # Flesch Reading Ease\n",
        "  flesch = 206.835 - (1.015 * avg_sentence_length) - (84.6 * (count_total_syllables_text(text) / total_words))\n",
        "\n",
        "  return np.array([avg_sentence_length, avg_word_length, stopword_ratio, lexical_diversity, total_punctuation_occurrences, num_unique_punctuation_types, avg_punc, flesch], dtype=np.float32)"
      ],
      "metadata": {
        "id": "QyeBZYM9GA2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = '/content/drive/My Drive/UofT/APS360 - Project/Data' # Different for everyone\n",
        "\n",
        "#--------- Loading the datasets ------------\n",
        "human_train, human_val, human_test, ai_train, ai_val, ai_test = loadTrainValTestData(data_path)"
      ],
      "metadata": {
        "id": "uaZv-Q9qYVkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.concat([human_train, ai_train], ignore_index=True)\n",
        "val_df = pd.concat([human_val, ai_val], ignore_index=True)\n",
        "test_df = pd.concat([human_test, ai_test], ignore_index=True)\n",
        "\n",
        "# Extracting features\n",
        "X_train = np.stack(train_df['text_clean'].apply(extractFeatures))\n",
        "y_train = train_df['label'].values\n",
        "\n",
        "X_val = np.stack(val_df['text_clean'].apply(extractFeatures))\n",
        "y_val = val_df['label'].values\n",
        "\n",
        "# Standarizing the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)"
      ],
      "metadata": {
        "id": "30B7kKg9ZbXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving and Loading the Extracted Features\n",
        "To avoid reruning the extraction function"
      ],
      "metadata": {
        "id": "g4n20wg0MVqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def saveToNumpy(data_path, file, name):\n",
        "  np.save(os.path.join(data_path, f'{name}.npy'), file)\n",
        "\n",
        "def loadFromNumpy(data_path, name):\n",
        "  return np.load(os.path.join(data_path, f'{name}.npy'))"
      ],
      "metadata": {
        "id": "DCW-JrxzMT-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp_path = '/content/drive/My Drive/UofT/APS360 - Project/Data/variables'\n",
        "saveToNumpy(temp_path, X_train, 'X_train')\n",
        "saveToNumpy(temp_path, X_train_scaled, 'X_train_scaled')\n",
        "saveToNumpy(temp_path, y_train, 'y_train')\n",
        "saveToNumpy(temp_path, X_val, 'X_val')\n",
        "saveToNumpy(temp_path, X_val_scaled, 'X_val_scaled')\n",
        "saveToNumpy(temp_path, y_val, 'y_val')"
      ],
      "metadata": {
        "id": "SBv_1h0lMXwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp_path = '/content/drive/My Drive/UofT/APS360 - Project/Data/variables'\n",
        "X_train2 = loadFromNumpy(temp_path, 'X_train')\n",
        "X_train_scaled2 = loadFromNumpy(temp_path, 'X_train_scaled')\n",
        "y_train2 = loadFromNumpy(temp_path, 'y_train')\n",
        "X_val2 = loadFromNumpy(temp_path, 'X_val')\n",
        "X_val_scaled2 = loadFromNumpy(temp_path, 'X_val_scaled')\n",
        "y_val2 = loadFromNumpy(temp_path, 'y_val')"
      ],
      "metadata": {
        "id": "635NEKmkMZV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Baseline Model**"
      ],
      "metadata": {
        "id": "jJ6q8j1_Zszy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters set based on proposal\n",
        "# Build the model\n",
        "svm = SVC(kernel='rbf', C=1.0, gamma='scale')\n",
        "# Trained the model\n",
        "svm.fit(X_train_scaled2, y_train2)\n",
        "\n",
        "# Validation\n",
        "val_preds = svm.predict(X_val_scaled2)\n",
        "print(\"Validation Results:\\n\", classification_report(y_val2, val_preds, target_names=[\"Human\", \"AI\"]))"
      ],
      "metadata": {
        "id": "ntgaHH7xZvlJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e3b2370-149d-4812-edcf-5a1ab5fa3320"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Results:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "       Human       0.82      0.95      0.88     12111\n",
            "          AI       0.89      0.63      0.73      6813\n",
            "\n",
            "    accuracy                           0.84     18924\n",
            "   macro avg       0.85      0.79      0.81     18924\n",
            "weighted avg       0.84      0.84      0.83     18924\n",
            "\n"
          ]
        }
      ]
    }
  ]
}